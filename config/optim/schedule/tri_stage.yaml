# the scheduler object to use
scheduler:
  _target_: torch.optim.lr_scheduler.LambdaLR

  # A function which computes a multiplicative factor given an integer parameter
  lr_lambda:
    _target_: src.optim.schedule.tri_stage.TriStageLearningRateLambdaLRFunction
    max_steps: ${trainer.max_steps}
    warmup_stage_ratio: 0.05
    constant_stage_ratio: 0.45
    decay_stage_ratio: 0.5
    initial_lr: ${divide:${optim.algo.lr},100}
    base_lr: ${optim.algo.lr}
    final_lr: ${divide:${optim.algo.lr},20}

  # epoch number after which to not do any steps any more. '-1' implies never stop
  last_epoch: -1

  # print to STDOUT when making a step
  verbose: false

# optional value to track which is fed into the step() call
# only relevant for learning rate schedulers such
# as `reduce on plateau`
monitor: null

# whether to step every epoch or every step
interval: step

# amount of epochs/steps between consecutive step() calls
frequency: null

# name to log the learning rate as
name: null