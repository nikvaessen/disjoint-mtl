to_add:
  - lr_monitor
  - ram_monitor
  - checkpoint
  - pbar
  - summary
  - early_stopping_speech
  - early_stopping_speech_diverge
  - early_stopping_speaker

# progress bar
pbar:
  _target_: pytorch_lightning.callbacks.RichProgressBar
  leave: true

# model summary
summary:
  _target_: pytorch_lightning.callbacks.ModelSummary
  max_depth: 5

# keep track of learning rate in logger
lr_monitor:
  _target_: pytorch_lightning.callbacks.LearningRateMonitor

ram_monitor:
  _target_: src.callbacks.memory_monitor.RamMemoryMonitor
  frequency: 100

# save model checkpoint of weights with best validation performance
checkpoint:
  _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
  monitor: val_loss
  save_top_k: 1
  mode: min
  filename: 'epoch_{epoch:04d}.step_{step:09d}.val-loss_{val_loss:.10f}.best'
  save_last: true
  every_n_epochs: 1
  save_on_train_epoch_end: false
  auto_insert_metric_name: false
  save_weights_only: true

last_checkpoint_pattern: 'epoch_{epoch:04d}.step_{step:09d}.val-loss_{val_loss:.10f}.best'

# stop when val_wer_other doesn't improve
early_stopping_speech:
  _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
  monitor: val_loss
  min_delta: 0.00
  patience: 8
  mode: min
  check_finite: True
  verbose: true
