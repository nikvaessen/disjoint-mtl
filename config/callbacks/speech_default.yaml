to_add:
  - pbar
  - summary
  - lr_monitor
  - ram_monitor
  - magnitude_monitor
  - checkpoint

# progress bar
pbar:
  _target_: pytorch_lightning.callbacks.RichProgressBar
  leave: true


# model summary
summary:
  _target_: pytorch_lightning.callbacks.ModelSummary
  max_depth: 4

# keep track of learning rate in logger
lr_monitor:
  _target_: pytorch_lightning.callbacks.LearningRateMonitor

# keep track of RAM usage
ram_monitor:
  _target_: src.callbacks.memory_monitor.RamMemoryMonitor
  frequency: 100

# keep track of magnitudes of weights and gradients
magnitude_monitor:
  _target_: src.callbacks.magnitude_monitor.MagnitudeMonitor
  frequency: 100

# save model checkpoint of weights with best validation performance
checkpoint:
  _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
  monitor: val_loss
  save_top_k: 1
  mode: min
  filename: 'epoch_{epoch:04d}.step_{step:09d}.val-loss_{val_loss:.10f}.best'
  save_last: true
  every_n_epochs: 1
  save_on_train_epoch_end: false
  auto_insert_metric_name: false
  save_weights_only: true

last_checkpoint_pattern: 'epoch_{epoch:04d}.step_{step:09d}.val-loss_{val_loss:.10f}.best'
