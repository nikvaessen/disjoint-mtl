# Instructions for setting up the LibriSpeech dataset

## Setting up the environment

The following environment variables should be set before continuing:

```bash
# example values
LIBRISPEECH_ROOT_DIR=${PWD}/data/librispeech/
LIBRISPEECH_RAW_DIR=${LIBRISPEECH_ROOT_DIR}/raw
LIBRISPEECH_EXTRACT_DIR=${LIBRISPEECH_ROOT_DIR}/extract
LIBRISPEECH_SHARD_DIR=${LIBRISPEECH_ROOT_DIR}/shards
LIBRISPEECH_META_DIR=${LIBRISPEECH_ROOT_DIR}/meta
```

## Downloading

Having set `LIBRISPEECH_RAW_DIR`, call:

```bash
./librispeech/download_librispeech.sh
```

## Extracting

After having downloaded the dataset, and having set `LIBRISPEECH_EXTRACT_DIR` and `LIBRISPEECH_META_DIR`, call:

```bash
./librispeech/untar_librispeech_archives.sh
```

## Convert to WAV

Having extracted the dataset, you can convert it to wav format by calling:

```bash
NUM_CPU=$(nproc) # or fewer if you want to use the PC for other stuff...
poetry run convert_to_wav --dir "$LIBRISPEECH_EXTRACT_DIR" --ext .flac --workers="$NUM_CPU"
```

## Write shards

In order to write shards of the data, we must first generate multiple csv files which
describe each split. These csv files are put into a separate folder for each split. 

### default split

In the default split, we have 3 train splits ('clean-100', 'clean-360' and 'other-500), 
as well as 2 dev splits (clean and other) and 2 test splits (clean and other)

Make sure `LIBRISPEECH_SHARD_DIR` is set.

First, call to generate a CSV file for each split :

```bash
./librispeech/default_setup_shards.sh
```

Then, in order to create shards for each split, call:

```bash
./librispeech/default_write_shards.sh
```

#### Generation of additional files

All steps below can also be run with `./librispeech/default_generate_meta_files.sh`.

For a speech recognition system, it is useful to know all possible characters in the
transcriptions, and to map these characters to a particular index.
This can be generated by the following command:

```bash
poetry run generate_character_vocabulary \
  "$LIBRISPEECH_SHARD_DIR"/train-clean-100 \
  "$LIBRISPEECH_SHARD_DIR"/train-clean-360 \
  "$LIBRISPEECH_SHARD_DIR"/train-other-500 \
  "$LIBRISPEECH_SHARD_DIR"/val-clean-100 \
  "$LIBRISPEECH_SHARD_DIR"/val-clean-360 \
  "$LIBRISPEECH_SHARD_DIR"/val-other-500 \
  "$LIBRISPEECH_SHARD_DIR"/dev-clean \
  "$LIBRISPEECH_SHARD_DIR"/dev-other \
  "$LIBRISPEECH_SHARD_DIR"/test-clean \
  "$LIBRISPEECH_SHARD_DIR"/test-other \
  --out "$LIBRISPEECH_META_DIR"/character_vocabulary.json
```

For speaker recognition systems, it is useful to have:

1. For training data, we want to map each speaker identity to a particular index
2. For all evaluation data, we want to generate trial lists

First, we will generate a mapping of all training speakers to a particular index:

```bash
poetry run generate_speaker_mapping \
  "$LIBRISPEECH_SHARD_DIR"/train-clean-100 \
  "$LIBRISPEECH_SHARD_DIR"/train-clean-360 \
  "$LIBRISPEECH_SHARD_DIR"/train-other-500 \
  "$LIBRISPEECH_SHARD_DIR"/val-clean-100 \
  "$LIBRISPEECH_SHARD_DIR"/val-clean-360 \
  "$LIBRISPEECH_SHARD_DIR"/val-other-500 \
  --out "$LIBRISPEECH_META_DIR"/train_speakers.json
```

Then, for each of val, dev, and test sets, we will create a trial list:

```bash
poetry run generate_speaker_trials "$LIBRISPEECH_SHARD_DIR"/val-* --out "$LIBRISPEECH_META_DIR"/trials_val.txt
poetry run generate_speaker_trials "$LIBRISPEECH_SHARD_DIR"/dev-clean  --out "$LIBRISPEECH_META_DIR"/trials_dev_clean.txt
poetry run generate_speaker_trials "$LIBRISPEECH_SHARD_DIR"/dev-other  --out "$LIBRISPEECH_META_DIR"/trials_dev_other.txt
poetry run generate_speaker_trials "$LIBRISPEECH_SHARD_DIR"/test-clean --out "$LIBRISPEECH_META_DIR"/trials_test_clean.txt
poetry run generate_speaker_trials "$LIBRISPEECH_SHARD_DIR"/test-other --out "$LIBRISPEECH_META_DIR"/trials_test_other.txt
```

